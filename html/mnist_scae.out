Python 2.7.13 (default, Jan 13 2017, 10:15:16)
Type "copyright", "credits" or "license" for more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: %logstart mnist_scae.log
Activating auto-logging. Current session state plus future input saved.
Filename       : mnist_scae.log
Mode           : backup
Output logging : False
Raw input log  : False
Timestamping   : False
State          : active

In [2]: runfile('/data/PROJECTS/THESISProject/CODE/mnist_scae.py', wdir='/data/PROJECTS/THESISProject/CODE')

Main information

[ALGORITHM] Momentum

[OPTION] batch_size = 64
[OPTION] verbose = True
[OPTION] epoch_end_signal = None
[OPTION] show_epoch = 1
[OPTION] shuffle_data = True
[OPTION] step = 0.1
[OPTION] train_end_signal = None
[OPTION] error = binary_crossentropy
[OPTION] addons = None
[OPTION] momentum = 0.99
[OPTION] nesterov = False

[THEANO] Initializing Theano variables and functions.
WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.
[THEANO] Initialization finished successfully. It took 57.98 seconds

Network's architecture

--------------------------------------------------
| #  | Input shape  | Layer Type  | Output shape |
--------------------------------------------------
| 1  | (1, 28, 28)  | Input       | (1, 28, 28)  |
| 2  | (1, 28, 28)  | Convolution | (16, 26, 26) |
| 3  | (16, 26, 26) | Relu        | (16, 26, 26) |
| 4  | (16, 26, 26) | Convolution | (16, 24, 24) |
| 5  | (16, 24, 24) | Relu        | (16, 24, 24) |
| 6  | (16, 24, 24) | MaxPooling  | (16, 12, 12) |
| 7  | (16, 12, 12) | Convolution | (32, 10, 10) |
| 8  | (32, 10, 10) | Relu        | (32, 10, 10) |
| 9  | (32, 10, 10) | MaxPooling  | (32, 5, 5)   |
| 10 | (32, 5, 5)   | Reshape     | 800          |
| 11 | 800          | Relu        | 256          |
| 12 | 256          | Relu        | 128          |
| 13 | 128          | Relu        | 256          |
| 14 | 256          | Relu        | 800          |
| 15 | 800          | Reshape     | (32, 5, 5)   |
| 16 | (32, 5, 5)   | Upscale     | (32, 10, 10) |
| 17 | (32, 10, 10) | Convolution | (16, 12, 12) |
| 18 | (16, 12, 12) | Relu        | (16, 12, 12) |
| 19 | (16, 12, 12) | Upscale     | (16, 24, 24) |
| 20 | (16, 24, 24) | Convolution | (16, 26, 26) |
| 21 | (16, 26, 26) | Relu        | (16, 26, 26) |
| 22 | (16, 26, 26) | Convolution | (1, 28, 28)  |
| 23 | (1, 28, 28)  | Sigmoid     | (1, 28, 28)  |
| 24 | (1, 28, 28)  | Reshape     | 784          |
--------------------------------------------------


Start training

[TRAINING DATA] shapes: (69000, 1, 28, 28)
[TEST DATA] shapes: (1000, 1, 28, 28)
[TRAINING] Total epochs: 10

------------------------------------------------
| Epoch # | Train err | Valid err | Time       |
------------------------------------------------
| 1       | 0.1635    | 0.1296    | 00:06:14   |                                         
| 2       | 0.1175    | 0.09911   | 00:06:13   |                                         
| 3       | 0.09611   | 0.09141   | 00:06:16   |                                         
| 4       | 0.09085   | 0.09229   | 00:06:12   |                                         
| 5       | 0.08792   | 0.08646   | 00:06:11   |                                         
| 6       | 0.0856    | 0.08514   | 00:06:11   |                                         
| 7       | 0.08384   | 0.08154   | 00:06:11   |                                         
| 8       | 0.08335   | 0.08141   | 00:06:17   |                                         
| 9       | 0.08182   | 0.08025   | 00:06:26   |                                         
| 10      | 0.08104   | 0.07964   | 00:06:23   |                                         
------------------------------------------------


Main information

[ALGORITHM] Adadelta

[OPTION] batch_size = 64
[OPTION] verbose = True
[OPTION] epoch_end_signal = None
[OPTION] show_epoch = 1
[OPTION] shuffle_data = True
[OPTION] step = 0.05
[OPTION] train_end_signal = None
[OPTION] error = categorical_crossentropy
[OPTION] addons = None
[OPTION] decay = 0.95
[OPTION] epsilon = 1e-05

[THEANO] Initializing Theano variables and functions.
[THEANO] Initialization finished successfully. It took 80.96 seconds

Network's architecture

-----------------------------------------------
| # | Input shape | Layer Type | Output shape |
-----------------------------------------------
| 1 | 128         | Input      | 128          |
| 2 | 128         | PRelu      | 512          |
| 3 | 512         | Dropout    | 512          |
| 4 | 512         | Softmax    | 10           |
-----------------------------------------------


Start training

[TRAINING DATA] shapes: (1000, 128)
[TEST DATA] shapes: (69000, 128)
[TRAINING] Total epochs: 100

------------------------------------------------
| Epoch # | Train err | Valid err | Time       |
------------------------------------------------
| 1       | 2.198     | 1.406     | 1.3 sec    |                                           
| 2       | 1.281     | 0.9019    | 0.9 sec    |                                           
| 3       | 0.9045    | 0.6848    | 0.9 sec    |                                           
| 4       | 0.7287    | 0.6011    | 0.9 sec    |                                           
| 5       | 0.5991    | 0.4985    | 0.9 sec    |                                           
| 6       | 0.5271    | 0.4665    | 0.9 sec    |                                          
| 7       | 0.4792    | 0.4403    | 0.9 sec    |                                           
| 8       | 0.4234    | 0.4044    | 0.9 sec    |                                           
| 9       | 0.4009    | 0.4166    | 0.9 sec    |                                           
| 10      | 0.3739    | 0.3799    | 0.9 sec    |                                           
| 11      | 0.3569    | 0.3875    | 1.0 sec    |                                          
------------------------------------------------
| Too many outputs in the terminal. Set up     |
| logging after each 2 epochs                  |
------------------------------------------------
| 12      | 0.3418    | 0.3521    | 0.9 sec    |
| 14      | 0.2929    | 0.3284    | 0.9 sec    |
| 16      | 0.2683    | 0.3336    | 0.9 sec    |
| 18      | 0.2747    | 0.3096    | 0.9 sec    |
| 20      | 0.2479    | 0.3192    | 0.9 sec    |
| 22      | 0.2216    | 0.3248    | 0.9 sec    |
| 24      | 0.2152    | 0.2921    | 0.9 sec    |
| 26      | 0.2213    | 0.3051    | 0.9 sec    |
| 28      | 0.2022    | 0.2941    | 0.9 sec    |
| 30      | 0.1875    | 0.2898    | 0.9 sec    |
| 32      | 0.1757    | 0.2834    | 0.9 sec    |
| 34      | 0.1583    | 0.2986    | 0.9 sec    |
| 36      | 0.1619    | 0.2897    | 0.9 sec    |
| 38      | 0.1681    | 0.3001    | 0.9 sec    |
| 40      | 0.1471    | 0.2958    | 0.9 sec    |
| 42      | 0.1429    | 0.2916    | 0.9 sec    |
| 44      | 0.1357    | 0.2924    | 0.9 sec    |
| 46      | 0.1277    | 0.2849    | 0.9 sec    |
| 48      | 0.1249    | 0.2882    | 0.9 sec    |
| 50      | 0.1266    | 0.2907    | 0.9 sec    |
| 52      | 0.111     | 0.2875    | 0.9 sec    |
| 54      | 0.1037    | 0.2791    | 0.9 sec    |
| 56      | 0.1076    | 0.2806    | 0.8 sec    |
| 58      | 0.09945   | 0.2857    | 0.9 sec    |
| 60      | 0.1022    | 0.2726    | 0.9 sec    |
| 62      | 0.09858   | 0.2939    | 0.9 sec    |
| 64      | 0.08475   | 0.2805    | 0.9 sec    |
| 66      | 0.08926   | 0.2833    | 0.9 sec    |
| 68      | 0.08364   | 0.2818    | 0.9 sec    |
| 70      | 0.07728   | 0.2885    | 0.8 sec    |
| 72      | 0.08936   | 0.2826    | 0.9 sec    |
| 74      | 0.08158   | 0.2873    | 0.9 sec    |
| 76      | 0.07381   | 0.2825    | 0.9 sec    |
| 78      | 0.07041   | 0.2891    | 0.9 sec    |
| 80      | 0.07356   | 0.3123    | 0.9 sec    |
| 82      | 0.06935   | 0.2803    | 0.9 sec    |
| 84      | 0.065     | 0.2811    | 0.9 sec    |
| 86      | 0.06394   | 0.2859    | 0.8 sec    |
| 88      | 0.05579   | 0.3059    | 0.9 sec    |
| 90      | 0.06155   | 0.2883    | 0.9 sec    |
| 92      | 0.05989   | 0.2847    | 0.9 sec    |
| 94      | 0.05568   | 0.2848    | 0.9 sec    |
| 96      | 0.05968   | 0.2821    | 0.9 sec    |
| 98      | 0.05251   | 0.283     | 0.8 sec    |
| 100     | 0.05796   | 0.2954    | 0.9 sec    |
------------------------------------------------


Main information

[ALGORITHM] MinibatchGradientDescent

[OPTION] batch_size = 64
[OPTION] verbose = True
[OPTION] epoch_end_signal = None
[OPTION] show_epoch = 1
[OPTION] shuffle_data = True
[OPTION] step = 0.01
[OPTION] train_end_signal = None
[OPTION] error = categorical_crossentropy
[OPTION] addons = None

[THEANO] Initializing Theano variables and functions.
[THEANO] Initialization finished successfully. It took 10.32 seconds

Network's architecture

--------------------------------------------------
| #  | Input shape  | Layer Type  | Output shape |
--------------------------------------------------
| 1  | (1, 28, 28)  | Input       | (1, 28, 28)  |
| 2  | (1, 28, 28)  | Convolution | (16, 26, 26) |
| 3  | (16, 26, 26) | Relu        | (16, 26, 26) |
| 4  | (16, 26, 26) | Convolution | (16, 24, 24) |
| 5  | (16, 24, 24) | Relu        | (16, 24, 24) |
| 6  | (16, 24, 24) | MaxPooling  | (16, 12, 12) |
| 7  | (16, 12, 12) | Convolution | (32, 10, 10) |
| 8  | (32, 10, 10) | Relu        | (32, 10, 10) |
| 9  | (32, 10, 10) | MaxPooling  | (32, 5, 5)   |
| 10 | (32, 5, 5)   | Reshape     | 800          |
| 11 | 800          | Relu        | 256          |
| 12 | 256          | Relu        | 128          |
| 13 | 128          | PRelu       | 512          |
| 14 | 512          | Dropout     | 512          |
| 15 | 512          | Softmax     | 10           |
--------------------------------------------------


Start training

[TRAINING DATA] shapes: (1000, 1, 28, 28)
[TRAINING] Total epochs: 100

------------------------------------------------
| Epoch # | Train err | Valid err | Time       |
------------------------------------------------
| 1       | 0.05037   | -         | 1.9 sec    |                                     
| 2       | 0.04385   | -         | 1.9 sec    |                                     
| 3       | 0.04316   | -         | 1.9 sec    |                                     
| 4       | 0.04174   | -         | 2.0 sec    |                                     
| 5       | 0.0407    | -         | 1.9 sec    |                                     
| 6       | 0.03339   | -         | 1.9 sec    |                                     
| 7       | 0.03395   | -         | 1.9 sec    |                                     
| 8       | 0.02954   | -         | 1.9 sec    |                                     
| 9       | 0.03098   | -         | 1.9 sec    |                                     
| 10      | 0.02777   | -         | 1.9 sec    |                                     
| 11      | 0.03105   | -         | 1.9 sec    |                                     
| 12      | 0.03119   | -         | 1.9 sec    |                                     
| 13      | 0.03335   | -         | 1.9 sec    |                                     
| 14      | 0.0253    | -         | 1.9 sec    |                                     
| 15      | 0.02271   | -         | 1.9 sec    |                                     
| 16      | 0.0251    | -         | 1.9 sec    |                                     
| 17      | 0.02319   | -         | 1.9 sec    |                                     
| 18      | 0.02013   | -         | 1.9 sec    |                                     
| 19      | 0.02007   | -         | 1.9 sec    |                                     
| 20      | 0.02204   | -         | 1.9 sec    |                                     
| 21      | 0.01978   | -         | 1.9 sec    |                                     
| 22      | 0.01984   | -         | 1.9 sec    |                                     
| 23      | 0.01654   | -         | 1.9 sec    |                                     
| 24      | 0.02149   | -         | 1.9 sec    |                                     
| 25      | 0.019     | -         | 1.9 sec    |                                     
| 26      | 0.02022   | -         | 1.9 sec    |                                     
| 27      | 0.02046   | -         | 1.9 sec    |                                     
| 28      | 0.0145    | -         | 1.9 sec    |                                     
| 29      | 0.01667   | -         | 1.9 sec    |                                     
| 30      | 0.01622   | -         | 1.9 sec    |                                     
| 31      | 0.0156    | -         | 1.9 sec    |                                     
| 32      | 0.01939   | -         | 2.0 sec    |                                     
| 33      | 0.01429   | -         | 1.9 sec    |                                     
| 34      | 0.01563   | -         | 1.9 sec    |                                     
| 35      | 0.01499   | -         | 1.9 sec    |                                     
| 36      | 0.01529   | -         | 1.9 sec    |                                     
| 37      | 0.01631   | -         | 1.9 sec    |                                     
| 38      | 0.0118    | -         | 1.9 sec    |                                     
| 39      | 0.01445   | -         | 1.9 sec    |                                     
| 40      | 0.01178   | -         | 1.9 sec    |                                     
| 41      | 0.0101    | -         | 1.9 sec    |                                     
| 42      | 0.01269   | -         | 1.9 sec    |                                     
| 43      | 0.01327   | -         | 1.9 sec    |                                     
| 44      | 0.01506   | -         | 1.9 sec    |                                     
| 45      | 0.01074   | -         | 1.9 sec    |                                     
| 46      | 0.01755   | -         | 1.9 sec    |                                     
| 47      | 0.01307   | -         | 1.9 sec    |                                     
| 48      | 0.01224   | -         | 1.9 sec    |                                     
| 49      | 0.009214  | -         | 1.9 sec    |                                     
| 50      | 0.01579   | -         | 1.9 sec    |                                     
| 51      | 0.01707   | -         | 1.9 sec    |                                     
| 52      | 0.005577  | -         | 1.9 sec    |                                     
| 53      | 0.01054   | -         | 1.9 sec    |                                     
| 54      | 0.007161  | -         | 1.9 sec    |                                     
| 55      | 0.01278   | -         | 1.9 sec    |                                     
| 56      | 0.01063   | -         | 1.9 sec    |                                     
| 57      | 0.01118   | -         | 1.9 sec    |                                     
| 58      | 0.008392  | -         | 1.9 sec    |                                     
| 59      | 0.01072   | -         | 1.9 sec    |                                     
| 60      | 0.009802  | -         | 1.9 sec    |                                     
| 61      | 0.01233   | -         | 1.9 sec    |                                     
| 62      | 0.007043  | -         | 1.9 sec    |                                     
| 63      | 0.00855   | -         | 1.9 sec    |                                     
| 64      | 0.009714  | -         | 1.9 sec    |                                     
| 65      | 0.006979  | -         | 1.9 sec    |                                     
| 66      | 0.006738  | -         | 1.9 sec    |                                     
| 67      | 0.008334  | -         | 1.9 sec    |                                     
| 68      | 0.008652  | -         | 2.0 sec    |                                     
| 69      | 0.005969  | -         | 1.9 sec    |                                     
| 70      | 0.00791   | -         | 1.9 sec    |                                     
| 71      | 0.007605  | -         | 1.9 sec    |                                     
| 72      | 0.01201   | -         | 1.9 sec    |                                     
| 73      | 0.005141  | -         | 1.9 sec    |                                     
| 74      | 0.004443  | -         | 1.9 sec    |                                     
| 75      | 0.007549  | -         | 1.9 sec    |                                     
| 76      | 0.009455  | -         | 1.9 sec    |                                     
| 77      | 0.006891  | -         | 1.9 sec    |                                     
| 78      | 0.009288  | -         | 1.9 sec    |                                     
| 79      | 0.00513   | -         | 1.9 sec    |                                     
| 80      | 0.0079    | -         | 1.9 sec    |                                     
| 81      | 0.004418  | -         | 1.9 sec    |                                     
| 82      | 0.009845  | -         | 1.9 sec    |                                     
| 83      | 0.004869  | -         | 1.9 sec    |                                     
| 84      | 0.003479  | -         | 1.9 sec    |                                     
| 85      | 0.00579   | -         | 1.9 sec    |                                     
| 86      | 0.008607  | -         | 1.9 sec    |                                     
| 87      | 0.004355  | -         | 1.9 sec    |                                     
| 88      | 0.004664  | -         | 1.9 sec    |                                     
| 89      | 0.0088    | -         | 1.9 sec    |                                     
| 90      | 0.004155  | -         | 1.9 sec    |                                     
| 91      | 0.01222   | -         | 1.9 sec    |                                     
| 92      | 0.006445  | -         | 1.9 sec    |                                     
| 93      | 0.00418   | -         | 1.9 sec    |                                     
| 94      | 0.004977  | -         | 1.9 sec    |                                     
| 95      | 0.007343  | -         | 1.9 sec    |                                     
| 96      | 0.004728  | -         | 1.9 sec    |                                     
| 97      | 0.005547  | -         | 1.9 sec    |                                     
| 98      | 0.004404  | -         | 1.9 sec    |                                     
| 99      | 0.003772  | -         | 1.9 sec    |                                     
| 100     | 0.005673  | -         | 1.9 sec    |                                     
------------------------------------------------

             precision    recall  f1-score   support                        

          0       0.94      0.97      0.95      6810
          1       0.95      0.98      0.97      7764
          2       0.91      0.90      0.91      6903
          3       0.93      0.89      0.91      7045
          4       0.92      0.93      0.92      6726
          5       0.91      0.89      0.90      6231
          6       0.93      0.95      0.94      6763
          7       0.93      0.92      0.93      7181
          8       0.92      0.87      0.89      6730
          9       0.88      0.93      0.91      6847

avg / total       0.92      0.92      0.92     69000

Validation accuracy: 92.33%

In [3]: 
